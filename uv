#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
make_caseA_table_from_GESLA4_headers.py

Build a LaTeX table of Case A (class==1) stations, enriched with country/lat/lon
parsed from the first 41 header lines of the raw GESLA4 files in ~/GESLA4/DATA/.

Inputs:
  ~/WORKSHOP/esbjerg-storm-surge-extremes-publication/FIGURES/GOF_GEV_BOOTSTRAP/pp_boot_scores.csv

GESLA4 raw files:
  ~/GESLA4/DATA/<station_id_without__annual>

Outputs:
  FIGURES/GOF_GEV_BOOTSTRAP/caseA_station_metadata_from_GESLA4.csv
  FIGURES/GOF_GEV_BOOTSTRAP/caseA_table.tex

Usage:
  uv run make_caseA_table_from_GESLA4_headers.py
  uv run make_caseA_table_from_GESLA4_headers.py --limit 20
"""

from __future__ import annotations

import argparse
import re
from pathlib import Path
from typing import Dict, Optional, Tuple, List

import numpy as np
import pandas as pd


BASE_DIR = Path.home() / "WORKSHOP/esbjerg-storm-surge-extremes-publication"
SCORES_CSV = BASE_DIR / "FIGURES" / "GOF_GEV_BOOTSTRAP" / "pp_boot_scores.csv"
OUT_DIR = BASE_DIR / "FIGURES" / "GOF_GEV_BOOTSTRAP"

GESLA4_DIR = Path.home() / "GESLA4" / "DATA"
HEADER_LINES = 41


# ----------------------------
# Header parsing helpers
# ----------------------------
def _normalise_key(s: str) -> str:
    return re.sub(r"\s+", " ", s.strip()).upper()


def parse_header_lines(lines: List[str]) -> Dict[str, str]:
    """
    Parse GESLA-like header lines that look like:
      KEYWORD .... value
    We keep it permissive: split on multiple spaces.
    """
    d: Dict[str, str] = {}
    for raw in lines:
        line = raw.rstrip("\n")
        if not line.strip():
            continue
        # Many GESLA headers are "KEY  value" with variable spaces.
        parts = re.split(r"\s{2,}", line.strip(), maxsplit=1)
        if len(parts) == 2:
            key, val = parts
            d[_normalise_key(key)] = val.strip()
        else:
            # fallback: some headers may use ":" or "="
            m = re.match(r"^([^:=]+)[:=]\s*(.+)$", line.strip())
            if m:
                d[_normalise_key(m.group(1))] = m.group(2).strip()
    return d


def pick_first(d: Dict[str, str], keys: List[str]) -> Optional[str]:
    for k in keys:
        kk = _normalise_key(k)
        if kk in d:
            return d[kk]
    return None


def as_float(s: Optional[str]) -> Optional[float]:
    if s is None:
        return None
    # Extract first float-like number
    m = re.search(r"[-+]?\d+(?:\.\d+)?", s.replace(",", "."))
    if not m:
        return None
    try:
        return float(m.group(0))
    except Exception:
        return None


def as_int(s: Optional[str]) -> Optional[int]:
    if s is None:
        return None
    m = re.search(r"[-+]?\d+", s)
    if not m:
        return None
    try:
        return int(m.group(0))
    except Exception:
        return None


def find_gesla_file(station_annual: str, gesla_dir: Path) -> Tuple[Optional[Path], str]:
    """
    station_annual example:
      'honolulu-1612340-usa-noaa_annual'
    raw gesla name usually:
      'honolulu-1612340-usa-noaa'

    We try:
      1) exact match by stripping '_annual'
      2) prefix glob (strip '_annual' and match start)
    """
    raw = station_annual
    if raw.endswith("_annual"):
        raw = raw[: -len("_annual")]

    # 1) exact
    p = gesla_dir / raw
    if p.exists() and p.is_file():
        return p, "exact"

    # 2) prefix glob (handles rare naming drift)
    # e.g. raw might be missing a suffix; match any file starting with raw
    hits = sorted(gesla_dir.glob(raw + "*"))
    hits = [h for h in hits if h.is_file()]
    if len(hits) == 1:
        return hits[0], "prefix-unique"
    if len(hits) > 1:
        # Prefer the shortest filename (often the base station id)
        hits.sort(key=lambda x: (len(x.name), x.name))
        return hits[0], f"prefix-multi({len(hits)})"

    return None, "not-found"


def read_header(path: Path, n_lines: int = HEADER_LINES) -> Dict[str, str]:
    with path.open("r", encoding="utf-8", errors="replace") as f:
        lines = [next(f) for _ in range(n_lines)]
    return parse_header_lines(lines)


# ----------------------------
# LaTeX table formatting
# ----------------------------
def latex_escape_minimal(s: str) -> str:
    # minimal escaping for table text fields; keep hyphens etc.
    # (we DO escape underscores which break LaTeX)
    return (
        s.replace("\\", "\\textbackslash{}")
        .replace("_", "\\_")
        .replace("%", "\\%")
        .replace("&", "\\&")
        .replace("#", "\\#")
    )


def main() -> None:
    ap = argparse.ArgumentParser()
    ap.add_argument("--scores", type=str, default=str(SCORES_CSV), help="pp_boot_scores.csv path")
    ap.add_argument("--gesla_dir", type=str, default=str(GESLA4_DIR), help="GESLA4/DATA directory")
    ap.add_argument("--limit", type=int, default=0, help="Limit number of Case A stations (0=all)")
    args = ap.parse_args()

    scores_path = Path(args.scores).expanduser()
    gesla_dir = Path(args.gesla_dir).expanduser()

    if not scores_path.exists():
        raise SystemExit(f"Scores CSV not found: {scores_path}")
    if not gesla_dir.exists():
        raise SystemExit(f"GESLA directory not found: {gesla_dir}")

    df = pd.read_csv(scores_path)
    needed = {"station", "filename", "class", "n", "beta1_NS", "deltaE", "lrt_p", "NS_selected_LRT_5pct", "E_S", "E_NS"}
    missing = sorted(list(needed - set(df.columns)))
    if missing:
        raise SystemExit(f"Scores CSV missing required columns: {missing}")

    caseA = df[df["class"] == 1].copy()
    if args.limit and args.limit > 0:
        caseA = caseA.head(args.limit)

    print(f"Case A stations: {len(caseA)}")

    rows = []
    not_found = 0

    for _, r in caseA.iterrows():
        station = str(r["station"])
        fp, how = find_gesla_file(station, gesla_dir)

        meta = {
            "station": station,
            "annual_filename": str(r["filename"]),
            "gesla_file": str(fp) if fp else "",
            "gesla_match": how,
            "lat": np.nan,
            "lon": np.nan,
            "country": "",
            "station_name": "",
            "gauge_type": "",
            "start_year": np.nan,
            "end_year": np.nan,
            "n_years_header": np.nan,
        }

        if fp is None:
            not_found += 1
        else:
            try:
                h = read_header(fp, HEADER_LINES)

                meta["station_name"] = pick_first(h, ["STATION NAME", "STATION", "NAME"]) or ""
                meta["country"] = pick_first(h, ["COUNTRY"]) or ""
                meta["gauge_type"] = pick_first(h, ["GAUGE TYPE"]) or ""

                meta["lat"] = as_float(pick_first(h, ["LATITUDE", "LAT"])) or np.nan
                meta["lon"] = as_float(pick_first(h, ["LONGITUDE", "LON"])) or np.nan

                meta["start_year"] = as_int(pick_first(h, ["START YEAR", "START"])) or np.nan
                meta["end_year"] = as_int(pick_first(h, ["END YEAR", "END"])) or np.nan
                meta["n_years_header"] = as_int(pick_first(h, ["NUMBER OF YEARS", "YEARS"])) or np.nan

            except Exception as e:
                meta["gesla_match"] = f"{how}-readfail"
                meta["gesla_file"] = str(fp)
                meta["station_name"] = f"HEADER_READ_FAILED: {e}"

        # Add stats from scores
        meta.update({
            "n_annual": int(r["n"]),
            "beta1_NS": float(r["beta1_NS"]),
            "deltaE": float(r["deltaE"]),
            "E_S": float(r["E_S"]),
            "E_NS": float(r["E_NS"]),
            "lrt_p": float(r["lrt_p"]),
            "NS_selected_LRT_5pct": int(r["NS_selected_LRT_5pct"]),
        })

        rows.append(meta)

    out_meta = pd.DataFrame(rows)
    OUT_DIR.mkdir(parents=True, exist_ok=True)

    meta_csv = OUT_DIR / "caseA_station_metadata_from_GESLA4.csv"
    out_meta.to_csv(meta_csv, index=False)
    print(f"Wrote: {meta_csv}")
    if not_found:
        print(f"WARNING: {not_found} Case A stations could not be matched to a file in {gesla_dir}")

    # Build LaTeX table
    tab = out_meta.copy()

    # Use station_name if available; otherwise station id
    def display_name(row):
        nm = str(row.get("station_name", "")).strip()
        if nm and not nm.startswith("HEADER_READ_FAILED"):
            return nm
        return str(row["station"])

    tab["Station"] = tab.apply(display_name, axis=1)
    tab["Country"] = tab["country"].fillna("").astype(str)
    tab["Lat"] = tab["lat"].round(2)
    tab["Lon"] = tab["lon"].round(2)
    tab["Years"] = tab["n_annual"].astype(int)
    tab[r"$\hat{\beta}_1$"] = tab["beta1_NS"].round(3)
    tab[r"$\Delta E$"] = tab["deltaE"].round(3)
    tab[r"$E_S$"] = tab["E_S"].round(3)
    tab[r"$E_{NS}$"] = tab["E_NS"].round(3)
    tab["LRT(5\\%)"] = tab["NS_selected_LRT_5pct"].map({0: "S", 1: "NS"})

    tab = tab[[
        "Station", "Country", "Lat", "Lon", "Years",
        r"$\hat{\beta}_1$", r"$\Delta E$", r"$E_S$", r"$E_{NS}$", "LRT(5\\%)"
    ]].copy()

    # Sort: strongest positive beta first (you can change this)
    tab = tab.sort_values(by=r"$\hat{\beta}_1$", ascending=False)

    # Escape text columns for LaTeX safety
    for col in ["Station", "Country", "LRT(5\\%)"]:
        tab[col] = tab[col].astype(str).map(latex_escape_minimal)

    tex_path = OUT_DIR / "caseA_table.tex"

    latex = tab.to_latex(
        index=False,
        escape=False,
        longtable=True,          # Case A has 53 rows -> longtable is appropriate
        caption=(
            "Stations classified as Case~A (non-stationary model improves calibration), "
            "augmented with metadata parsed from the first 41 header lines of the corresponding GESLA4 files. "
            "Here $\\hat{\\beta}_1$ is the estimated mean-sea-level effect on the GEV location parameter, "
            "$\\Delta E = E_S - E_{NS}$ is the reduction in the fraction of PP points outside the 95\\% "
            "parametric-bootstrap envelope when moving from S to NS, and $E_S$ and $E_{NS}$ are the envelope-exceedance "
            "fractions for the two models."
        ),
        label="tab:caseA_stations",
        column_format="p{4.6cm} p{2.6cm} r r r r r r r c",
        na_rep="",
    )

    # Add booktabs + a little spacing tweak for longtable
    # pandas already uses top/mid/bottomrule when booktabs is installed (in LaTeX).
    with tex_path.open("w", encoding="utf-8") as f:
        f.write(latex)

    print(f"Wrote: {tex_path}")
    print("Done.")


if __name__ == "__main__":
    main()

